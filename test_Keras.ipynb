{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 15:54:48.229483 10396 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 15:54:48.260406 10396 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 15:55:11.368244 10396 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0723 15:55:11.393211 10396 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74166691 0.70987158 0.78570384 ... 0.80473515 0.98504668 0.2750797 ]\n",
      " [0.89076776 0.26707435 0.38701443 ... 0.4604779  0.81752711 0.99405299]\n",
      " [0.87623624 0.15981996 0.31366024 ... 0.62348022 0.63639584 0.47429301]\n",
      " ...\n",
      " [0.43334804 0.75619214 0.42524293 ... 0.52037165 0.29683875 0.93832215]\n",
      " [0.00848367 0.42552524 0.35193385 ... 0.84323673 0.9563285  0.68324048]\n",
      " [0.13258425 0.1306022  0.86685157 ... 0.66103012 0.92169474 0.75181969]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 11:50:28.569626 16000 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0724 11:50:28.831403 16000 deprecation.py:506] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0724 11:50:28.932114 16000 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0724 11:50:28.957216 16000 deprecation_wrapper.py:119] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0724 11:50:29.483873 16000 deprecation.py:323] From C:\\Users\\sunny.DESKTOP-QGFGEEK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.4073 - acc: 0.1090\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3720 - acc: 0.0960\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 2.3517 - acc: 0.1050\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 2.3256 - acc: 0.1030\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3238 - acc: 0.1010\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 19us/step - loss: 2.3207 - acc: 0.1210\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 2.3141 - acc: 0.1090\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3089 - acc: 0.1090\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 2.3079 - acc: 0.1050\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 2.3046 - acc: 0.1160\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 2.3110 - acc: 0.0990\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.3120 - acc: 0.1080\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 2.3028 - acc: 0.1120\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3081 - acc: 0.0970\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3058 - acc: 0.1060\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3012 - acc: 0.1050\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 2.3020 - acc: 0.1040\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 2.3100 - acc: 0.0910\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3029 - acc: 0.1130\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 21us/step - loss: 2.3004 - acc: 0.1120\n",
      "100/100 [==============================] - 0s 799us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv prepare data set\n",
    "dataset_source = 'C:/Users/sunny.DESKTOP-QGFGEEK/Desktop/GitClone/1_camera_gait_analysis/matched_excel/walkR1.csv'\n",
    "dataset_df = pd.read_csv(dataset_source,sep = ',',engine='python')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1390625  0.54076087 0.128125   0.63858696 0.0859375  0.74456522]\n",
      " [0.1484375  0.53804348 0.1375     0.63858696 0.09375    0.73913043]\n",
      " [0.1546875  0.53804348 0.1421875  0.63586957 0.096875   0.73913043]\n",
      " [0.159375   0.53532609 0.1484375  0.63586957 0.1015625  0.73641304]\n",
      " [0.165625   0.53532609 0.1546875  0.63586957 0.10625    0.73369565]\n",
      " [0.171875   0.53532609 0.1609375  0.63586957 0.1109375  0.73369565]\n",
      " [0.184375   0.53532609 0.1734375  0.63586957 0.1234375  0.72826087]\n",
      " [0.190625   0.53532609 0.18125    0.63586957 0.128125   0.72554348]\n",
      " [0.196875   0.53532609 0.1875     0.63586957 0.1359375  0.72554348]\n",
      " [0.203125   0.53532609 0.1953125  0.63586957 0.1421875  0.72554348]\n",
      " [0.209375   0.53532609 0.203125   0.63586957 0.15       0.72282609]\n",
      " [0.221875   0.53532609 0.2171875  0.63586957 0.1640625  0.72282609]\n",
      " [0.228125   0.53532609 0.225      0.63586957 0.171875   0.72282609]\n",
      " [0.234375   0.5326087  0.2328125  0.63586957 0.178125   0.7201087 ]\n",
      " [0.240625   0.5326087  0.2390625  0.63315217 0.1859375  0.7173913 ]\n",
      " [0.246875   0.5298913  0.2453125  0.63043478 0.19375    0.7173913 ]\n",
      " [0.259375   0.52717391 0.2609375  0.62771739 0.209375   0.71467391]\n",
      " [0.265625   0.52717391 0.2671875  0.62771739 0.2171875  0.71195652]\n",
      " [0.271875   0.52445652 0.2734375  0.625      0.225      0.71195652]\n",
      " [0.278125   0.52445652 0.28125    0.625      0.2328125  0.71195652]\n",
      " [0.2828125  0.52173913 0.2875     0.62228261 0.240625   0.71195652]\n",
      " [0.29375    0.51902174 0.3        0.61684783 0.25625    0.71195652]\n",
      " [0.2984375  0.51630435 0.30625    0.61684783 0.2640625  0.71195652]\n",
      " [0.2984375  0.51630435 0.30625    0.61684783 0.2640625  0.71195652]\n",
      " [0.3078125  0.51358696 0.3171875  0.61141304 0.278125   0.71195652]\n",
      " [0.3140625  0.51358696 0.3234375  0.61141304 0.2859375  0.71195652]\n",
      " [0.3234375  0.51086957 0.334375   0.60869565 0.3        0.71195652]\n",
      " [0.3265625  0.50815217 0.3390625  0.60597826 0.3078125  0.71467391]\n",
      " [0.3265625  0.50815217 0.3390625  0.60597826 0.3078125  0.71467391]\n",
      " [0.3359375  0.50815217 0.35       0.60597826 0.321875   0.71467391]\n",
      " [0.3390625  0.50815217 0.3546875  0.60326087 0.3296875  0.7173913 ]\n",
      " [0.3484375  0.50543478 0.3640625  0.60326087 0.34375    0.7173913 ]\n",
      " [0.3484375  0.50543478 0.3640625  0.60326087 0.34375    0.7173913 ]\n",
      " [0.3515625  0.50543478 0.3671875  0.60326087 0.35       0.7173913 ]\n",
      " [0.359375   0.50543478 0.3765625  0.60326087 0.3625     0.7201087 ]\n",
      " [0.3625     0.50543478 0.3796875  0.60054348 0.3703125  0.7201087 ]\n",
      " [0.365625   0.50543478 0.384375   0.60326087 0.3765625  0.7201087 ]\n",
      " [0.36875    0.50815217 0.3875     0.60326087 0.38125    0.72282609]\n",
      " [0.3734375  0.50815217 0.390625   0.60326087 0.3890625  0.72282609]\n",
      " [0.3796875  0.51086957 0.3984375  0.60326087 0.4        0.72282609]\n",
      " [0.3828125  0.51086957 0.4015625  0.60326087 0.4046875  0.72282609]\n",
      " [0.384375   0.51086957 0.4046875  0.60326087 0.4109375  0.72282609]\n",
      " [0.3875     0.51358696 0.4078125  0.60597826 0.415625   0.72282609]\n",
      " [0.390625   0.51630435 0.4109375  0.60597826 0.4203125  0.72282609]\n",
      " [0.3984375  0.51902174 0.4171875  0.60597826 0.4296875  0.72282609]\n",
      " [0.4        0.51902174 0.421875   0.60869565 0.4359375  0.72282609]\n",
      " [0.403125   0.52173913 0.425      0.60869565 0.4390625  0.72282609]\n",
      " [0.40625    0.52173913 0.428125   0.60869565 0.44375    0.72282609]\n",
      " [0.409375   0.52445652 0.43125    0.60869565 0.4484375  0.72554348]\n",
      " [0.4171875  0.52445652 0.4390625  0.61141304 0.45625    0.72554348]\n",
      " [0.421875   0.52445652 0.4421875  0.61141304 0.4609375  0.72554348]\n",
      " [0.425      0.52445652 0.4453125  0.61141304 0.4640625  0.72826087]\n",
      " [0.4296875  0.52717391 0.45       0.61413043 0.4671875  0.72826087]\n",
      " [0.434375   0.52717391 0.4546875  0.61413043 0.4703125  0.72826087]\n",
      " [0.44375    0.52717391 0.4625     0.61413043 0.4765625  0.73097826]\n",
      " [0.446875   0.52717391 0.4671875  0.61413043 0.48125    0.73097826]\n",
      " [0.453125   0.52717391 0.4703125  0.61413043 0.484375   0.73369565]\n",
      " [0.45625    0.52717391 0.4765625  0.61684783 0.4875     0.73369565]\n",
      " [0.4625     0.52717391 0.4796875  0.61684783 0.490625   0.73369565]\n",
      " [0.4703125  0.52717391 0.4875     0.61684783 0.4953125  0.73641304]\n",
      " [0.475      0.52717391 0.4921875  0.61956522 0.4984375  0.73641304]\n",
      " [0.4796875  0.52717391 0.4953125  0.61956522 0.5015625  0.73641304]\n",
      " [0.4828125  0.52717391 0.5        0.61684783 0.5046875  0.73641304]\n",
      " [0.4875     0.52717391 0.5046875  0.61684783 0.509375   0.73641304]\n",
      " [0.4953125  0.52717391 0.5125     0.61413043 0.51875    0.73369565]\n",
      " [0.5        0.52717391 0.5171875  0.61413043 0.5234375  0.73641304]\n",
      " [0.5046875  0.52717391 0.521875   0.61684783 0.5265625  0.73641304]\n",
      " [0.5078125  0.52717391 0.525      0.61684783 0.528125   0.73641304]\n",
      " [0.5109375  0.52717391 0.528125   0.61684783 0.5296875  0.73641304]\n",
      " [0.51875    0.52717391 0.5328125  0.61684783 0.5328125  0.73641304]\n",
      " [0.5203125  0.52717391 0.534375   0.61956522 0.5328125  0.73913043]\n",
      " [0.5234375  0.5298913  0.5359375  0.61956522 0.534375   0.73913043]\n",
      " [0.525      0.5298913  0.5375     0.61956522 0.5359375  0.73913043]\n",
      " [0.5265625  0.5298913  0.5390625  0.61956522 0.5359375  0.73913043]\n",
      " [0.53125    0.5298913  0.54375    0.62228261 0.5390625  0.73913043]\n",
      " [0.534375   0.5298913  0.5453125  0.62228261 0.5390625  0.73913043]\n",
      " [0.5359375  0.5298913  0.546875   0.62228261 0.540625   0.73913043]\n",
      " [0.5390625  0.5298913  0.5484375  0.62228261 0.5421875  0.73913043]\n",
      " [0.540625   0.5298913  0.55       0.62228261 0.5421875  0.73913043]\n",
      " [0.5453125  0.5298913  0.553125   0.62228261 0.54375    0.73913043]\n",
      " [0.5484375  0.5298913  0.5546875  0.62228261 0.5453125  0.73913043]\n",
      " [0.55       0.5298913  0.55625    0.62228261 0.5453125  0.74184783]\n",
      " [0.5515625  0.5298913  0.55625    0.62228261 0.5453125  0.73913043]\n",
      " [0.553125   0.5298913  0.5578125  0.62228261 0.546875   0.74184783]\n",
      " [0.55625    0.5298913  0.559375   0.62228261 0.546875   0.73913043]\n",
      " [0.5578125  0.5298913  0.559375   0.62228261 0.5484375  0.73913043]\n",
      " [0.559375   0.5298913  0.5609375  0.625      0.5484375  0.74184783]\n",
      " [0.5609375  0.5298913  0.5609375  0.62228261 0.5484375  0.73913043]\n",
      " [0.5625     0.5298913  0.5625     0.625      0.55       0.73913043]\n",
      " [0.565625   0.5298913  0.5640625  0.625      0.55       0.74184783]\n",
      " [0.5671875  0.5298913  0.565625   0.625      0.5515625  0.73913043]\n",
      " [0.56875    0.5298913  0.565625   0.625      0.5515625  0.73913043]\n",
      " [0.5703125  0.5298913  0.5671875  0.62228261 0.5515625  0.73913043]\n",
      " [0.571875   0.5298913  0.5671875  0.62228261 0.553125   0.73913043]\n",
      " [0.575      0.5298913  0.56875    0.625      0.553125   0.73913043]\n",
      " [0.5765625  0.5298913  0.5703125  0.625      0.553125   0.73913043]\n",
      " [0.578125   0.5298913  0.5703125  0.625      0.5546875  0.73913043]\n",
      " [0.5796875  0.5298913  0.571875   0.625      0.5546875  0.73913043]\n",
      " [0.58125    0.5298913  0.571875   0.625      0.5546875  0.73913043]\n",
      " [0.584375   0.5298913  0.575      0.625      0.55625    0.73913043]\n",
      " [0.5859375  0.5298913  0.575      0.625      0.55625    0.73913043]\n",
      " [0.5875     0.5298913  0.5765625  0.625      0.5578125  0.73913043]\n",
      " [0.590625   0.5298913  0.578125   0.625      0.5578125  0.73913043]\n",
      " [0.5921875  0.5298913  0.578125   0.625      0.5578125  0.73913043]\n",
      " [0.5953125  0.5326087  0.58125    0.625      0.559375   0.73913043]\n",
      " [0.5953125  0.5326087  0.58125    0.625      0.559375   0.73913043]\n",
      " [0.5984375  0.5326087  0.584375   0.625      0.5609375  0.73913043]\n",
      " [0.6015625  0.5326087  0.5859375  0.625      0.5625     0.73641304]\n",
      " [0.603125   0.5326087  0.5875     0.625      0.5625     0.73641304]\n",
      " [0.6046875  0.5326087  0.5890625  0.62771739 0.5640625  0.73913043]\n",
      " [0.60625    0.5326087  0.590625   0.62771739 0.565625   0.73641304]\n",
      " [0.6109375  0.53532609 0.59375    0.62771739 0.5671875  0.73641304]\n",
      " [0.6125     0.53532609 0.5953125  0.62771739 0.56875    0.73641304]\n",
      " [0.615625   0.53532609 0.596875   0.62771739 0.56875    0.73641304]\n",
      " [0.6171875  0.53532609 0.6        0.62771739 0.5703125  0.73641304]\n",
      " [0.6203125  0.53532609 0.6015625  0.62771739 0.571875   0.73369565]\n",
      " [0.625      0.53532609 0.6046875  0.62771739 0.575      0.73369565]\n",
      " [0.628125   0.53532609 0.6078125  0.62771739 0.5765625  0.73369565]\n",
      " [0.63125    0.53532609 0.6109375  0.62771739 0.578125   0.73369565]\n",
      " [0.6328125  0.53532609 0.6125     0.62771739 0.5796875  0.73097826]\n",
      " [0.6359375  0.53532609 0.615625   0.62771739 0.5828125  0.73097826]\n",
      " [0.6421875  0.53532609 0.621875   0.62771739 0.5859375  0.72826087]\n",
      " [0.646875   0.53532609 0.625      0.625      0.5890625  0.72826087]\n",
      " [0.65       0.5326087  0.628125   0.625      0.590625   0.72554348]\n",
      " [0.653125   0.5326087  0.63125    0.625      0.59375    0.72554348]\n",
      " [0.6578125  0.5326087  0.6359375  0.625      0.596875   0.72282609]\n",
      " [0.6671875  0.5326087  0.6453125  0.62228261 0.603125   0.7201087 ]\n",
      " [0.6703125  0.5298913  0.65       0.62228261 0.60625    0.7173913 ]]\n"
     ]
    }
   ],
   "source": [
    "#crate numpy array of input\n",
    "input_array = pd.DataFrame(dataset_df.iloc[:,:6]).to_numpy()\n",
    "print(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.87067083]\n",
      " [0.85612335]\n",
      " [0.83936787]\n",
      " [0.82047685]\n",
      " [0.79954282]\n",
      " [0.77668477]\n",
      " [0.75204932]\n",
      " [0.72581093]\n",
      " [0.6981699 ]\n",
      " [0.66934798]\n",
      " [0.6395826 ]\n",
      " [0.60911732]\n",
      " [0.57818963]\n",
      " [0.54701347]\n",
      " [0.51576967]\n",
      " [0.48459327]\n",
      " [0.45357663]\n",
      " [0.42278048]\n",
      " [0.3922544 ]\n",
      " [0.36206505]\n",
      " [0.3323217 ]\n",
      " [0.30319713]\n",
      " [0.27493723]\n",
      " [0.24785868]\n",
      " [0.22233328]\n",
      " [0.19876187]\n",
      " [0.17754275]\n",
      " [0.15903873]\n",
      " [0.14354643]\n",
      " [0.13127568]\n",
      " [0.12233383]\n",
      " [0.11672042]\n",
      " [0.11432872]\n",
      " [0.11495303]\n",
      " [0.11830332]\n",
      " [0.12402638]\n",
      " [0.13173647]\n",
      " [0.14104778]\n",
      " [0.15161543]\n",
      " [0.16316493]\n",
      " [0.1755213 ]\n",
      " [0.1886167 ]\n",
      " [0.20248015]\n",
      " [0.217197  ]\n",
      " [0.23284605]\n",
      " [0.2494115 ]\n",
      " [0.26670262]\n",
      " [0.28431455]\n",
      " [0.30165882]\n",
      " [0.31804927]\n",
      " [0.33280868]\n",
      " [0.34535947]\n",
      " [0.3552918 ]\n",
      " [0.36239982]\n",
      " [0.36668408]\n",
      " [0.368332  ]\n",
      " [0.36767095]\n",
      " [0.36511115]\n",
      " [0.36108895]\n",
      " [0.3560132 ]\n",
      " [0.3502304 ]\n",
      " [0.34400817]\n",
      " [0.33753707]\n",
      " [0.33094118]\n",
      " [0.3242999 ]\n",
      " [0.31766392]\n",
      " [0.31107183]\n",
      " [0.30455832]\n",
      " [0.29815747]\n",
      " [0.29190233]\n",
      " [0.28582457]\n",
      " [0.27994923]\n",
      " [0.27429327]\n",
      " [0.26886583]\n",
      " [0.26366897]\n",
      " [0.2586971 ]\n",
      " [0.25394033]\n",
      " [0.24938223]\n",
      " [0.24500575]\n",
      " [0.24078952]\n",
      " [0.23671627]\n",
      " [0.23277795]\n",
      " [0.22898577]\n",
      " [0.22537743]\n",
      " [0.22202428]\n",
      " [0.21902973]\n",
      " [0.21652448]\n",
      " [0.21465198]\n",
      " [0.2135501 ]\n",
      " [0.21333125]\n",
      " [0.21406468]\n",
      " [0.21576555]\n",
      " [0.21838698]\n",
      " [0.22182612]\n",
      " [0.2259352 ]\n",
      " [0.23054155]\n",
      " [0.23546707]\n",
      " [0.24054712]\n",
      " [0.24564605]\n",
      " [0.25066428]\n",
      " [0.255542  ]\n",
      " [0.26026058]\n",
      " [0.26484103]\n",
      " [0.26934115]\n",
      " [0.2738562 ]\n",
      " [0.27851875]\n",
      " [0.28349632]\n",
      " [0.28899112]\n",
      " [0.29523742]\n",
      " [0.30249295]\n",
      " [0.31103438]\n",
      " [0.32114295]\n",
      " [0.33309533]\n",
      " [0.34715065]\n",
      " [0.36353757]\n",
      " [0.38244193]\n",
      " [0.40399407]\n",
      " [0.42825432]\n",
      " [0.4551989 ]\n",
      " [0.48470008]\n",
      " [0.51651353]\n",
      " [0.55026547]\n",
      " [0.58544445]\n",
      " [0.6214063 ]\n",
      " [0.65739345]\n",
      " [0.69256753]\n",
      " [0.7260657 ]\n",
      " [0.75706292]]\n"
     ]
    }
   ],
   "source": [
    "#crate numpy array of target\n",
    "target_array = pd.DataFrame(dataset_df.iloc[:,6:]).to_numpy()\n",
    "print(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3453 - acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.3146 - acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.2764 - acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.2426 - acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.2185 - acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.2046 - acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.1926 - acc: 0.0000e+00\n",
      "Epoch 8/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.1808 - acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.1777 - acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.1663 - acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.1565 - acc: 0.0000e+00\n",
      "Epoch 12/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.1494 - acc: 0.0000e+00\n",
      "Epoch 13/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.1430 - acc: 0.0000e+00\n",
      "Epoch 14/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.1355 - acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.1290 - acc: 0.0000e+00\n",
      "Epoch 16/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.1276 - acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.1201 - acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.1175 - acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.1144 - acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.1112 - acc: 0.0000e+00\n",
      "Epoch 21/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.1040 - acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.1024 - acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0999 - acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0942 - acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0932 - acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0881 - acc: 0.0000e+00\n",
      "Epoch 27/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0877 - acc: 0.0000e+00\n",
      "Epoch 28/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0839 - acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "128/128 [==============================] - 0s 226us/step - loss: 0.0820 - acc: 0.0000e+00\n",
      "Epoch 30/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0790 - acc: 0.0000e+00\n",
      "Epoch 31/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0767 - acc: 0.0000e+00\n",
      "Epoch 32/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0737 - acc: 0.0000e+00\n",
      "Epoch 33/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0731 - acc: 0.0000e+00\n",
      "Epoch 34/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0696 - acc: 0.0000e+00\n",
      "Epoch 35/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0677 - acc: 0.0000e+00\n",
      "Epoch 36/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0691 - acc: 0.0000e+00\n",
      "Epoch 37/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0647 - acc: 0.0000e+00\n",
      "Epoch 38/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0641 - acc: 0.0000e+00\n",
      "Epoch 39/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0616 - acc: 0.0000e+00\n",
      "Epoch 40/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0601 - acc: 0.0000e+00\n",
      "Epoch 41/150\n",
      "128/128 [==============================] - 0s 148us/step - loss: 0.0600 - acc: 0.0000e+00\n",
      "Epoch 42/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0583 - acc: 0.0000e+00\n",
      "Epoch 43/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0555 - acc: 0.0000e+00\n",
      "Epoch 44/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0578 - acc: 0.0000e+00\n",
      "Epoch 45/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0569 - acc: 0.0000e+00\n",
      "Epoch 46/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0551 - acc: 0.0000e+00\n",
      "Epoch 47/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0540 - acc: 0.0000e+00\n",
      "Epoch 48/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0516 - acc: 0.0000e+00\n",
      "Epoch 49/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0513 - acc: 0.0000e+00\n",
      "Epoch 50/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0508 - acc: 0.0000e+00\n",
      "Epoch 51/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0514 - acc: 0.0000e+00\n",
      "Epoch 52/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0517 - acc: 0.0000e+00\n",
      "Epoch 53/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0491 - acc: 0.0000e+00\n",
      "Epoch 54/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0484 - acc: 0.0000e+00\n",
      "Epoch 55/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0480 - acc: 0.0000e+00\n",
      "Epoch 56/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0472 - acc: 0.0000e+00\n",
      "Epoch 57/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0462 - acc: 0.0000e+00\n",
      "Epoch 58/150\n",
      "128/128 [==============================] - 0s 242us/step - loss: 0.0458 - acc: 0.0000e+00\n",
      "Epoch 59/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0457 - acc: 0.0000e+00\n",
      "Epoch 60/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0466 - acc: 0.0000e+00\n",
      "Epoch 61/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0450 - acc: 0.0000e+00\n",
      "Epoch 62/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0438 - acc: 0.0000e+00\n",
      "Epoch 63/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0447 - acc: 0.0000e+00\n",
      "Epoch 64/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0433 - acc: 0.0000e+00\n",
      "Epoch 65/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0437 - acc: 0.0000e+00\n",
      "Epoch 66/150\n",
      "128/128 [==============================] - ETA: 0s - loss: 0.0540 - acc: 0.0000e+0 - 0s 171us/step - loss: 0.0423 - acc: 0.0000e+00\n",
      "Epoch 67/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0416 - acc: 0.0000e+00\n",
      "Epoch 68/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0407 - acc: 0.0000e+00\n",
      "Epoch 69/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0403 - acc: 0.0000e+00\n",
      "Epoch 70/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0402 - acc: 0.0000e+00\n",
      "Epoch 71/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0407 - acc: 0.0000e+00\n",
      "Epoch 72/150\n",
      "128/128 [==============================] - 0s 172us/step - loss: 0.0392 - acc: 0.0000e+00\n",
      "Epoch 73/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0405 - acc: 0.0000e+00\n",
      "Epoch 74/150\n",
      "128/128 [==============================] - 0s 148us/step - loss: 0.0396 - acc: 0.0000e+00\n",
      "Epoch 75/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0394 - acc: 0.0000e+00\n",
      "Epoch 76/150\n",
      "128/128 [==============================] - 0s 148us/step - loss: 0.0395 - acc: 0.0000e+00\n",
      "Epoch 77/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0386 - acc: 0.0000e+00\n",
      "Epoch 78/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0390 - acc: 0.0000e+00\n",
      "Epoch 79/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0395 - acc: 0.0000e+00\n",
      "Epoch 80/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0391 - acc: 0.0000e+00\n",
      "Epoch 81/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0374 - acc: 0.0000e+00\n",
      "Epoch 82/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0375 - acc: 0.0000e+00\n",
      "Epoch 83/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0368 - acc: 0.0000e+00\n",
      "Epoch 84/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0386 - acc: 0.0000e+00\n",
      "Epoch 85/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0379 - acc: 0.0000e+00\n",
      "Epoch 86/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.0374 - acc: 0.0000e+00\n",
      "Epoch 87/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0383 - acc: 0.0000e+00\n",
      "Epoch 88/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0373 - acc: 0.0000e+00\n",
      "Epoch 89/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0370 - acc: 0.0000e+00\n",
      "Epoch 90/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0365 - acc: 0.0000e+00\n",
      "Epoch 91/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0370 - acc: 0.0000e+00\n",
      "Epoch 92/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0367 - acc: 0.0000e+00\n",
      "Epoch 93/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0367 - acc: 0.0000e+00\n",
      "Epoch 94/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0364 - acc: 0.0000e+00\n",
      "Epoch 95/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0374 - acc: 0.0000e+00\n",
      "Epoch 96/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0360 - acc: 0.0000e+00\n",
      "Epoch 97/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0351 - acc: 0.0000e+00\n",
      "Epoch 98/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0366 - acc: 0.0000e+00\n",
      "Epoch 99/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0358 - acc: 0.0000e+00\n",
      "Epoch 100/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0365 - acc: 0.0000e+00\n",
      "Epoch 101/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0346 - acc: 0.0000e+00\n",
      "Epoch 102/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.0353 - acc: 0.0000e+00\n",
      "Epoch 103/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0355 - acc: 0.0000e+00\n",
      "Epoch 104/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0350 - acc: 0.0000e+00\n",
      "Epoch 105/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0355 - acc: 0.0000e+00\n",
      "Epoch 106/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.0357 - acc: 0.0000e+00\n",
      "Epoch 107/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0352 - acc: 0.0000e+00\n",
      "Epoch 108/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0357 - acc: 0.0000e+00\n",
      "Epoch 109/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0353 - acc: 0.0000e+00\n",
      "Epoch 110/150\n",
      "128/128 [==============================] - 0s 132us/step - loss: 0.0344 - acc: 0.0000e+00\n",
      "Epoch 111/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0345 - acc: 0.0000e+00\n",
      "Epoch 112/150\n",
      "128/128 [==============================] - 0s 148us/step - loss: 0.0356 - acc: 0.0000e+00\n",
      "Epoch 113/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0347 - acc: 0.0000e+00\n",
      "Epoch 114/150\n",
      "128/128 [==============================] - 0s 218us/step - loss: 0.0344 - acc: 0.0000e+00\n",
      "Epoch 115/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0348 - acc: 0.0000e+00\n",
      "Epoch 116/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0349 - acc: 0.0000e+00\n",
      "Epoch 117/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0351 - acc: 0.0000e+00\n",
      "Epoch 118/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0350 - acc: 0.0000e+00\n",
      "Epoch 119/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0344 - acc: 0.0000e+00\n",
      "Epoch 120/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0344 - acc: 0.0000e+00\n",
      "Epoch 121/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0346 - acc: 0.0000e+00\n",
      "Epoch 122/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0339 - acc: 0.0000e+00\n",
      "Epoch 123/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0341 - acc: 0.0000e+00\n",
      "Epoch 124/150\n",
      "128/128 [==============================] - 0s 218us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 125/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0334 - acc: 0.0000e+00\n",
      "Epoch 126/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0352 - acc: 0.0000e+00\n",
      "Epoch 127/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0333 - acc: 0.0000e+00\n",
      "Epoch 128/150\n",
      "128/128 [==============================] - 0s 210us/step - loss: 0.0345 - acc: 0.0000e+00\n",
      "Epoch 129/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0341 - acc: 0.0000e+00\n",
      "Epoch 130/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0341 - acc: 0.0000e+00\n",
      "Epoch 131/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0343 - acc: 0.0000e+00\n",
      "Epoch 132/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0338 - acc: 0.0000e+00\n",
      "Epoch 133/150\n",
      "128/128 [==============================] - 0s 203us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 134/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0336 - acc: 0.0000e+00\n",
      "Epoch 135/150\n",
      "128/128 [==============================] - 0s 218us/step - loss: 0.0346 - acc: 0.0000e+00\n",
      "Epoch 136/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0342 - acc: 0.0000e+00\n",
      "Epoch 137/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 138/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0343 - acc: 0.0000e+00\n",
      "Epoch 139/150\n",
      "128/128 [==============================] - 0s 156us/step - loss: 0.0339 - acc: 0.0000e+00\n",
      "Epoch 140/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 141/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0335 - acc: 0.0000e+00\n",
      "Epoch 142/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 143/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0334 - acc: 0.0000e+00\n",
      "Epoch 144/150\n",
      "128/128 [==============================] - 0s 218us/step - loss: 0.0333 - acc: 0.0000e+00\n",
      "Epoch 145/150\n",
      "128/128 [==============================] - 0s 164us/step - loss: 0.0336 - acc: 0.0000e+00\n",
      "Epoch 146/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0340 - acc: 0.0000e+00\n",
      "Epoch 147/150\n",
      "128/128 [==============================] - 0s 179us/step - loss: 0.0336 - acc: 0.0000e+00\n",
      "Epoch 148/150\n",
      "128/128 [==============================] - 0s 171us/step - loss: 0.0333 - acc: 0.0000e+00\n",
      "Epoch 149/150\n",
      "128/128 [==============================] - 0s 187us/step - loss: 0.0336 - acc: 0.0000e+00\n",
      "Epoch 150/150\n",
      "128/128 [==============================] - 0s 195us/step - loss: 0.0341 - acc: 0.0000e+00\n",
      "128/128 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "#my code\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "#read data set\n",
    "\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=6,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(input_array, target_array, epochs=150, batch_size=10)\n",
    "\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(input_array,  target_array)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1390625, 0.5407608695652174, 0.128125, 0.6385869565217391, 0.0859375, 0.7445652173913043] => 0 (expected 0)\n",
      "[0.1484375, 0.5380434782608695, 0.1375, 0.6385869565217391, 0.09375, 0.7391304347826086] => 0 (expected 0)\n",
      "[0.1546875, 0.5380434782608695, 0.1421875, 0.6358695652173914, 0.096875, 0.7391304347826086] => 0 (expected 0)\n",
      "[0.159375, 0.5353260869565217, 0.1484375, 0.6358695652173914, 0.1015625, 0.7364130434782609] => 0 (expected 0)\n",
      "[0.165625, 0.5353260869565217, 0.1546875, 0.6358695652173914, 0.10625, 0.7336956521739131] => 0 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(input_array)\n",
    "# summarize the first 5 cases\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (input_array[i].tolist(), predictions[i], target_array[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import numpy \n",
    "import pandas as pd\n",
    "# load dataset\n",
    "yacht = pd.read_csv(dataset_source, names=[\"THIx\", \"THIy\", \"KNEx\", \"KNEy\", \"TIBx\", \"TIBy\", \"knee angle\"], sep=\",\")\n",
    "yacht.head()\n",
    "#%%\n",
    "yacht.isnull().values.any()\n",
    "\n",
    "#%%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = yacht.drop([\"knee angle\"], axis=1)\n",
    "y = yacht[\"knee angle\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "#%%\n",
    "x_train\n",
    "\n",
    "#%%\n",
    "x_train.shape\n",
    "\n",
    "#%%\n",
    "y_train\n",
    "\n",
    "#%%\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "#%%\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu',\n",
    "    input_shape=(x_train.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "k = 4\n",
    "num_val_samples = len(x_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "        x_train[(i + 1) * num_val_samples:]],axis=0\n",
    "    )\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "        y_train[(i + 1) * num_val_samples:]],axis=0\n",
    "    )\n",
    "    model = build_model()\n",
    "    model.fit(partial_x_train, partial_y_train,\n",
    "            epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "#%%\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_x_train = np.concatenate(\n",
    "        [x_train[:i * num_val_samples],\n",
    "        x_train[(i + 1) * num_val_samples:]],axis=0\n",
    "    )\n",
    "    partial_y_train = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "        y_train[(i + 1) * num_val_samples:]],axis=0\n",
    "    )\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_x_train, partial_y_train,\n",
    "            validation_data=(val_data, val_targets),\n",
    "            epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
